Command Line Args: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', 'detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl'], resume=False)
[32m[01/30 21:54:02 detectron2]: [0mRank of current process: 0. World size: 1
[32m[01/30 21:54:02 detectron2]: [0mEnvironment info:
------------------------  ---------------------------------------------------------------
sys.platform              linux
Python                    3.6.9 (default, Nov  7 2019, 10:44:02) [GCC 8.3.0]
numpy                     1.17.5
detectron2                0.1 @/content/detectron2_repo/detectron2
detectron2 compiler       GCC 7.4
detectron2 CUDA compiler  10.0
detectron2 arch flags     sm_75
DETECTRON2_ENV_MODULE     <not set>
PyTorch                   1.4.0+cu100 @/usr/local/lib/python3.6/dist-packages/torch
PyTorch debug build       False
CUDA available            True
GPU 0                     Tesla T4
CUDA_HOME                 /usr/local/cuda
NVCC                      Cuda compilation tools, release 10.0, V10.0.130
Pillow                    6.2.2
torchvision               0.5.0+cu100 @/usr/local/lib/python3.6/dist-packages/torchvision
torchvision arch flags    sm_35, sm_50, sm_60, sm_70, sm_75
cv2                       4.1.2
------------------------  ---------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - Intel(R) Math Kernel Library Version 2019.0.4 Product Build 20190411 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v0.21.1 (Git Hash 7d2fd500bc78936d1d648ca713b901012f470dbc)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CUDA Runtime 10.0
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.3
  - Magma 2.5.1
  - Build settings: BLAS=MKL, BUILD_NAMEDTENSOR=OFF, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -fopenmp -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Wno-stringop-overflow, DISABLE_NUMA=1, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_STATIC_DISPATCH=OFF, 

[32m[01/30 21:54:02 detectron2]: [0mCommand line arguments: Namespace(config_file='configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', 'detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl'], resume=False)
[32m[01/30 21:54:02 detectron2]: [0mContents of args.config_file=configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml:
_BASE_: "../Base-RCNN-FPN.yaml"
MODEL:
  WEIGHTS: "detectron2://ImageNetPretrained/MSRA/R-50.pkl"
  MASK_ON: True
  RESNETS:
    DEPTH: 50
SOLVER:
  STEPS: (210000, 250000)
  MAX_ITER: 270000

[32m[01/30 21:54:02 detectron2]: [0mRunning with full config:
CUDNN_BENCHMARK: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  FILTER_EMPTY_ANNOTATIONS: True
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: ()
  PROPOSAL_FILES_TRAIN: ()
  TEST: ('coco_2017_val',)
  TRAIN: ('coco_2017_train',)
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    ENABLED: False
    SIZE: [0.9, 0.9]
    TYPE: relative_range
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 1333
  MAX_SIZE_TRAIN: 1333
  MIN_SIZE_TEST: 800
  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN_SAMPLING: choice
MODEL:
  ANCHOR_GENERATOR:
    ANGLES: [[-90, 0, 90]]
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES: [[32], [64], [128], [256], [512]]
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_fpn_backbone
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']
    NORM: 
    OUT_CHANNELS: 256
  KEYPOINT_ON: False
  LOAD_PROPOSALS: False
  MASK_ON: True
  META_ARCHITECTURE: GeneralizedRCNN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: True
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN: [103.53, 116.28, 123.675]
  PIXEL_STD: [1.0, 1.0, 1.0]
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: False
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE: [False, False, False, False]
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: True
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.4, 0.5]
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))
    IOUS: (0.5, 0.6, 0.7)
  ROI_BOX_HEAD:
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    CLS_AGNOSTIC_BBOX_REG: False
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: FastRCNNConvFCHead
    NORM: 
    NUM_CONV: 0
    NUM_FC: 2
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    IOU_LABELS: [0, 1]
    IOU_THRESHOLDS: [0.5]
    NAME: StandardROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: True
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: False
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: 
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)
    BOUNDARY_THRESH: -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']
    IOU_LABELS: [0, -1, 1]
    IOU_THRESHOLDS: [0.3, 0.7]
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 1000
    PRE_NMS_TOPK_TEST: 1000
    PRE_NMS_TOPK_TRAIN: 2000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  WEIGHTS: detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl
OUTPUT_DIR: ./output
SEED: -1
SOLVER:
  BASE_LR: 0.02
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  GAMMA: 0.1
  IMS_PER_BATCH: 16
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 270000
  MOMENTUM: 0.9
  STEPS: (210000, 250000)
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1000
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0001
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: False
    FLIP: True
    MAX_SIZE: 4000
    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 0
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: False
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
[32m[01/30 21:54:02 detectron2]: [0mFull config saved to /content/detectron2_repo/output/config.yaml
[32m[01/30 21:54:02 d2.utils.env]: [0mUsing a generated random seed 2733923
[32m[01/30 21:54:07 d2.engine.defaults]: [0mModel:
GeneralizedRCNN(
  (backbone): FPN(
    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_block): LastLevelMaxPool()
    (bottom_up): ResNet(
      (stem): BasicStem(
        (conv1): Conv2d(
          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
        )
      )
      (res2): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv1): Conv2d(
            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv2): Conv2d(
            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)
          )
          (conv3): Conv2d(
            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
        )
      )
      (res3): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv1): Conv2d(
            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv2): Conv2d(
            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)
          )
          (conv3): Conv2d(
            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
        )
      )
      (res4): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
          (conv1): Conv2d(
            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (3): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (4): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
        (5): BottleneckBlock(
          (conv1): Conv2d(
            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv2): Conv2d(
            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)
          )
          (conv3): Conv2d(
            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)
          )
        )
      )
      (res5): Sequential(
        (0): BottleneckBlock(
          (shortcut): Conv2d(
            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
          (conv1): Conv2d(
            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (1): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
        (2): BottleneckBlock(
          (conv1): Conv2d(
            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv2): Conv2d(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)
          )
          (conv3): Conv2d(
            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False
            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)
          )
        )
      )
    )
  )
  (proposal_generator): RPN(
    (anchor_generator): DefaultAnchorGenerator(
      (cell_anchors): BufferList()
    )
    (rpn_head): StandardRPNHead(
      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): StandardROIHeads(
    (box_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (box_head): FastRCNNConvFCHead(
      (fc1): Linear(in_features=12544, out_features=1024, bias=True)
      (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNOutputLayers(
      (cls_score): Linear(in_features=1024, out_features=81, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)
    )
    (mask_pooler): ROIPooler(
      (level_poolers): ModuleList(
        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)
        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)
        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)
        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)
      )
    )
    (mask_head): MaskRCNNConvUpsampleHead(
      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
[32m[01/30 21:54:07 fvcore.common.checkpoint]: [0mLoading checkpoint from detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl
[32m[01/30 21:54:07 fvcore.common.file_io]: [0mDownloading https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...
[32m[01/30 21:54:07 fvcore.common.download]: [0mDownloading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...
[32m[01/30 21:54:18 fvcore.common.download]: [0mSuccessfully downloaded /root/.torch/fvcore_cache/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl. 177841981 bytes.
[32m[01/30 21:54:18 fvcore.common.file_io]: [0mURL https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl cached in /root/.torch/fvcore_cache/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl
[32m[01/30 21:54:18 fvcore.common.checkpoint]: [0mReading a file from 'Detectron2 Model Zoo'
[32m[01/30 21:54:18 d2.data.datasets.coco]: [0mLoaded 5000 images in COCO format from datasets/coco/annotations/instances_val2017.json
[32m[01/30 21:54:19 d2.data.build]: [0mDistribution of instances among all 80 categories:
[36m|   category    | #instances   |   category   | #instances   |   category    | #instances   |
|:-------------:|:-------------|:------------:|:-------------|:-------------:|:-------------|
|    person     | 10777        |   bicycle    | 314          |      car      | 1918         |
|  motorcycle   | 367          |   airplane   | 143          |      bus      | 283          |
|     train     | 190          |    truck     | 414          |     boat      | 424          |
| traffic light | 634          | fire hydrant | 101          |   stop sign   | 75           |
| parking meter | 60           |    bench     | 411          |     bird      | 427          |
|      cat      | 202          |     dog      | 218          |     horse     | 272          |
|     sheep     | 354          |     cow      | 372          |   elephant    | 252          |
|     bear      | 71           |    zebra     | 266          |    giraffe    | 232          |
|   backpack    | 371          |   umbrella   | 407          |    handbag    | 540          |
|      tie      | 252          |   suitcase   | 299          |    frisbee    | 115          |
|     skis      | 241          |  snowboard   | 69           |  sports ball  | 260          |
|     kite      | 327          | baseball bat | 145          | baseball gl.. | 148          |
|  skateboard   | 179          |  surfboard   | 267          | tennis racket | 225          |
|    bottle     | 1013         |  wine glass  | 341          |      cup      | 895          |
|     fork      | 215          |    knife     | 325          |     spoon     | 253          |
|     bowl      | 623          |    banana    | 370          |     apple     | 236          |
|   sandwich    | 177          |    orange    | 285          |   broccoli    | 312          |
|    carrot     | 365          |   hot dog    | 125          |     pizza     | 284          |
|     donut     | 328          |     cake     | 310          |     chair     | 1771         |
|     couch     | 261          | potted plant | 342          |      bed      | 163          |
| dining table  | 695          |    toilet    | 179          |      tv       | 288          |
|    laptop     | 231          |    mouse     | 106          |    remote     | 283          |
|   keyboard    | 153          |  cell phone  | 262          |   microwave   | 55           |
|     oven      | 143          |   toaster    | 9            |     sink      | 225          |
| refrigerator  | 126          |     book     | 1129         |     clock     | 267          |
|     vase      | 274          |   scissors   | 36           |  teddy bear   | 190          |
|  hair drier   | 11           |  toothbrush  | 57           |               |              |
|     total     | 36335        |              |              |               |              |[0m
[32m[01/30 21:54:20 d2.evaluation.evaluator]: [0mStart inference on 5000 images
[32m[01/30 21:54:22 d2.evaluation.evaluator]: [0mInference done 11/5000. 0.1118 s / img. ETA=0:11:54
[32m[01/30 21:54:27 d2.evaluation.evaluator]: [0mInference done 47/5000. 0.1125 s / img. ETA=0:11:38
[32m[01/30 21:54:32 d2.evaluation.evaluator]: [0mInference done 83/5000. 0.1140 s / img. ETA=0:11:40
[32m[01/30 21:54:37 d2.evaluation.evaluator]: [0mInference done 119/5000. 0.1132 s / img. ETA=0:11:38
[32m[01/30 21:54:43 d2.evaluation.evaluator]: [0mInference done 151/5000. 0.1144 s / img. ETA=0:11:51
[32m[01/30 21:54:48 d2.evaluation.evaluator]: [0mInference done 185/5000. 0.1144 s / img. ETA=0:11:48
[32m[01/30 21:54:53 d2.evaluation.evaluator]: [0mInference done 222/5000. 0.1141 s / img. ETA=0:11:34
[32m[01/30 21:54:58 d2.evaluation.evaluator]: [0mInference done 254/5000. 0.1147 s / img. ETA=0:11:36
[32m[01/30 21:55:03 d2.evaluation.evaluator]: [0mInference done 289/5000. 0.1149 s / img. ETA=0:11:32
[32m[01/30 21:55:08 d2.evaluation.evaluator]: [0mInference done 322/5000. 0.1151 s / img. ETA=0:11:29
[32m[01/30 21:55:13 d2.evaluation.evaluator]: [0mInference done 354/5000. 0.1156 s / img. ETA=0:11:28
[32m[01/30 21:55:18 d2.evaluation.evaluator]: [0mInference done 390/5000. 0.1155 s / img. ETA=0:11:21
[32m[01/30 21:55:23 d2.evaluation.evaluator]: [0mInference done 425/5000. 0.1154 s / img. ETA=0:11:14
[32m[01/30 21:55:28 d2.evaluation.evaluator]: [0mInference done 461/5000. 0.1156 s / img. ETA=0:11:07
[32m[01/30 21:55:33 d2.evaluation.evaluator]: [0mInference done 493/5000. 0.1160 s / img. ETA=0:11:05
[32m[01/30 21:55:38 d2.evaluation.evaluator]: [0mInference done 526/5000. 0.1160 s / img. ETA=0:11:02
[32m[01/30 21:55:43 d2.evaluation.evaluator]: [0mInference done 558/5000. 0.1162 s / img. ETA=0:10:59
[32m[01/30 21:55:48 d2.evaluation.evaluator]: [0mInference done 591/5000. 0.1164 s / img. ETA=0:10:56
[32m[01/30 21:55:53 d2.evaluation.evaluator]: [0mInference done 623/5000. 0.1169 s / img. ETA=0:10:54
[32m[01/30 21:55:59 d2.evaluation.evaluator]: [0mInference done 654/5000. 0.1171 s / img. ETA=0:10:52
[32m[01/30 21:56:04 d2.evaluation.evaluator]: [0mInference done 687/5000. 0.1172 s / img. ETA=0:10:48
[32m[01/30 21:56:09 d2.evaluation.evaluator]: [0mInference done 720/5000. 0.1174 s / img. ETA=0:10:44
[32m[01/30 21:56:14 d2.evaluation.evaluator]: [0mInference done 754/5000. 0.1175 s / img. ETA=0:10:38
[32m[01/30 21:56:19 d2.evaluation.evaluator]: [0mInference done 788/5000. 0.1176 s / img. ETA=0:10:33
[32m[01/30 21:56:24 d2.evaluation.evaluator]: [0mInference done 822/5000. 0.1177 s / img. ETA=0:10:28
[32m[01/30 21:56:29 d2.evaluation.evaluator]: [0mInference done 856/5000. 0.1178 s / img. ETA=0:10:23
[32m[01/30 21:56:34 d2.evaluation.evaluator]: [0mInference done 890/5000. 0.1179 s / img. ETA=0:10:17
[32m[01/30 21:56:39 d2.evaluation.evaluator]: [0mInference done 924/5000. 0.1179 s / img. ETA=0:10:12
[32m[01/30 21:56:44 d2.evaluation.evaluator]: [0mInference done 959/5000. 0.1180 s / img. ETA=0:10:06
[32m[01/30 21:56:49 d2.evaluation.evaluator]: [0mInference done 991/5000. 0.1182 s / img. ETA=0:10:03
[32m[01/30 21:56:55 d2.evaluation.evaluator]: [0mInference done 1026/5000. 0.1182 s / img. ETA=0:09:57
[32m[01/30 21:57:00 d2.evaluation.evaluator]: [0mInference done 1059/5000. 0.1183 s / img. ETA=0:09:53
[32m[01/30 21:57:05 d2.evaluation.evaluator]: [0mInference done 1092/5000. 0.1185 s / img. ETA=0:09:49
[32m[01/30 21:57:10 d2.evaluation.evaluator]: [0mInference done 1127/5000. 0.1185 s / img. ETA=0:09:42
[32m[01/30 21:57:15 d2.evaluation.evaluator]: [0mInference done 1159/5000. 0.1187 s / img. ETA=0:09:38
[32m[01/30 21:57:20 d2.evaluation.evaluator]: [0mInference done 1190/5000. 0.1189 s / img. ETA=0:09:35
[32m[01/30 21:57:25 d2.evaluation.evaluator]: [0mInference done 1221/5000. 0.1190 s / img. ETA=0:09:32
[32m[01/30 21:57:30 d2.evaluation.evaluator]: [0mInference done 1252/5000. 0.1192 s / img. ETA=0:09:28
[32m[01/30 21:57:35 d2.evaluation.evaluator]: [0mInference done 1285/5000. 0.1192 s / img. ETA=0:09:24
[32m[01/30 21:57:41 d2.evaluation.evaluator]: [0mInference done 1321/5000. 0.1192 s / img. ETA=0:09:17
[32m[01/30 21:57:46 d2.evaluation.evaluator]: [0mInference done 1351/5000. 0.1193 s / img. ETA=0:09:14
[32m[01/30 21:57:51 d2.evaluation.evaluator]: [0mInference done 1385/5000. 0.1194 s / img. ETA=0:09:08
[32m[01/30 21:57:56 d2.evaluation.evaluator]: [0mInference done 1421/5000. 0.1193 s / img. ETA=0:09:02
[32m[01/30 21:58:01 d2.evaluation.evaluator]: [0mInference done 1453/5000. 0.1194 s / img. ETA=0:08:58
[32m[01/30 21:58:06 d2.evaluation.evaluator]: [0mInference done 1487/5000. 0.1195 s / img. ETA=0:08:53
[32m[01/30 21:58:11 d2.evaluation.evaluator]: [0mInference done 1521/5000. 0.1194 s / img. ETA=0:08:48
[32m[01/30 21:58:16 d2.evaluation.evaluator]: [0mInference done 1553/5000. 0.1196 s / img. ETA=0:08:43
[32m[01/30 21:58:22 d2.evaluation.evaluator]: [0mInference done 1585/5000. 0.1197 s / img. ETA=0:08:39
[32m[01/30 21:58:27 d2.evaluation.evaluator]: [0mInference done 1616/5000. 0.1198 s / img. ETA=0:08:35
[32m[01/30 21:58:32 d2.evaluation.evaluator]: [0mInference done 1649/5000. 0.1198 s / img. ETA=0:08:31
[32m[01/30 21:58:37 d2.evaluation.evaluator]: [0mInference done 1682/5000. 0.1199 s / img. ETA=0:08:26
[32m[01/30 21:58:42 d2.evaluation.evaluator]: [0mInference done 1715/5000. 0.1200 s / img. ETA=0:08:21
[32m[01/30 21:58:47 d2.evaluation.evaluator]: [0mInference done 1748/5000. 0.1200 s / img. ETA=0:08:16
[32m[01/30 21:58:52 d2.evaluation.evaluator]: [0mInference done 1782/5000. 0.1200 s / img. ETA=0:08:10
[32m[01/30 21:58:57 d2.evaluation.evaluator]: [0mInference done 1816/5000. 0.1201 s / img. ETA=0:08:05
[32m[01/30 21:59:02 d2.evaluation.evaluator]: [0mInference done 1849/5000. 0.1201 s / img. ETA=0:08:00
[32m[01/30 21:59:07 d2.evaluation.evaluator]: [0mInference done 1881/5000. 0.1202 s / img. ETA=0:07:56
[32m[01/30 21:59:13 d2.evaluation.evaluator]: [0mInference done 1914/5000. 0.1203 s / img. ETA=0:07:51
[32m[01/30 21:59:18 d2.evaluation.evaluator]: [0mInference done 1950/5000. 0.1203 s / img. ETA=0:07:44
[32m[01/30 21:59:23 d2.evaluation.evaluator]: [0mInference done 1980/5000. 0.1204 s / img. ETA=0:07:41
[32m[01/30 21:59:28 d2.evaluation.evaluator]: [0mInference done 2012/5000. 0.1205 s / img. ETA=0:07:36
[32m[01/30 21:59:33 d2.evaluation.evaluator]: [0mInference done 2044/5000. 0.1205 s / img. ETA=0:07:32
[32m[01/30 21:59:38 d2.evaluation.evaluator]: [0mInference done 2080/5000. 0.1205 s / img. ETA=0:07:26
[32m[01/30 21:59:43 d2.evaluation.evaluator]: [0mInference done 2112/5000. 0.1206 s / img. ETA=0:07:21
[32m[01/30 21:59:48 d2.evaluation.evaluator]: [0mInference done 2146/5000. 0.1206 s / img. ETA=0:07:16
[32m[01/30 21:59:53 d2.evaluation.evaluator]: [0mInference done 2179/5000. 0.1206 s / img. ETA=0:07:11
[32m[01/30 21:59:58 d2.evaluation.evaluator]: [0mInference done 2212/5000. 0.1207 s / img. ETA=0:07:06
[32m[01/30 22:00:04 d2.evaluation.evaluator]: [0mInference done 2245/5000. 0.1208 s / img. ETA=0:07:01
[32m[01/30 22:00:09 d2.evaluation.evaluator]: [0mInference done 2277/5000. 0.1208 s / img. ETA=0:06:56
[32m[01/30 22:00:14 d2.evaluation.evaluator]: [0mInference done 2311/5000. 0.1209 s / img. ETA=0:06:51
[32m[01/30 22:00:19 d2.evaluation.evaluator]: [0mInference done 2346/5000. 0.1209 s / img. ETA=0:06:45
[32m[01/30 22:00:24 d2.evaluation.evaluator]: [0mInference done 2378/5000. 0.1210 s / img. ETA=0:06:40
[32m[01/30 22:00:29 d2.evaluation.evaluator]: [0mInference done 2409/5000. 0.1211 s / img. ETA=0:06:36
[32m[01/30 22:00:34 d2.evaluation.evaluator]: [0mInference done 2440/5000. 0.1211 s / img. ETA=0:06:32
[32m[01/30 22:00:39 d2.evaluation.evaluator]: [0mInference done 2472/5000. 0.1212 s / img. ETA=0:06:27
[32m[01/30 22:00:44 d2.evaluation.evaluator]: [0mInference done 2504/5000. 0.1212 s / img. ETA=0:06:22
[32m[01/30 22:00:49 d2.evaluation.evaluator]: [0mInference done 2535/5000. 0.1213 s / img. ETA=0:06:18
[32m[01/30 22:00:54 d2.evaluation.evaluator]: [0mInference done 2566/5000. 0.1213 s / img. ETA=0:06:13
[32m[01/30 22:00:59 d2.evaluation.evaluator]: [0mInference done 2597/5000. 0.1214 s / img. ETA=0:06:09
[32m[01/30 22:01:05 d2.evaluation.evaluator]: [0mInference done 2628/5000. 0.1215 s / img. ETA=0:06:04
[32m[01/30 22:01:10 d2.evaluation.evaluator]: [0mInference done 2660/5000. 0.1215 s / img. ETA=0:05:59
[32m[01/30 22:01:15 d2.evaluation.evaluator]: [0mInference done 2694/5000. 0.1216 s / img. ETA=0:05:54
[32m[01/30 22:01:20 d2.evaluation.evaluator]: [0mInference done 2727/5000. 0.1216 s / img. ETA=0:05:49
[32m[01/30 22:01:25 d2.evaluation.evaluator]: [0mInference done 2760/5000. 0.1216 s / img. ETA=0:05:44
[32m[01/30 22:01:30 d2.evaluation.evaluator]: [0mInference done 2792/5000. 0.1216 s / img. ETA=0:05:39
[32m[01/30 22:01:35 d2.evaluation.evaluator]: [0mInference done 2824/5000. 0.1217 s / img. ETA=0:05:34
[32m[01/30 22:01:40 d2.evaluation.evaluator]: [0mInference done 2857/5000. 0.1217 s / img. ETA=0:05:29
[32m[01/30 22:01:45 d2.evaluation.evaluator]: [0mInference done 2889/5000. 0.1218 s / img. ETA=0:05:25
[32m[01/30 22:01:50 d2.evaluation.evaluator]: [0mInference done 2920/5000. 0.1219 s / img. ETA=0:05:20
[32m[01/30 22:01:55 d2.evaluation.evaluator]: [0mInference done 2952/5000. 0.1219 s / img. ETA=0:05:15
[32m[01/30 22:02:00 d2.evaluation.evaluator]: [0mInference done 2986/5000. 0.1219 s / img. ETA=0:05:10
[32m[01/30 22:02:06 d2.evaluation.evaluator]: [0mInference done 3018/5000. 0.1220 s / img. ETA=0:05:05
[32m[01/30 22:02:11 d2.evaluation.evaluator]: [0mInference done 3050/5000. 0.1221 s / img. ETA=0:05:00
[32m[01/30 22:02:16 d2.evaluation.evaluator]: [0mInference done 3083/5000. 0.1222 s / img. ETA=0:04:55
[32m[01/30 22:02:21 d2.evaluation.evaluator]: [0mInference done 3117/5000. 0.1222 s / img. ETA=0:04:50
[32m[01/30 22:02:26 d2.evaluation.evaluator]: [0mInference done 3148/5000. 0.1222 s / img. ETA=0:04:45
[32m[01/30 22:02:31 d2.evaluation.evaluator]: [0mInference done 3180/5000. 0.1223 s / img. ETA=0:04:40
[32m[01/30 22:02:36 d2.evaluation.evaluator]: [0mInference done 3211/5000. 0.1223 s / img. ETA=0:04:36
[32m[01/30 22:02:41 d2.evaluation.evaluator]: [0mInference done 3242/5000. 0.1224 s / img. ETA=0:04:31
[32m[01/30 22:02:46 d2.evaluation.evaluator]: [0mInference done 3274/5000. 0.1224 s / img. ETA=0:04:26
[32m[01/30 22:02:51 d2.evaluation.evaluator]: [0mInference done 3305/5000. 0.1225 s / img. ETA=0:04:22
[32m[01/30 22:02:56 d2.evaluation.evaluator]: [0mInference done 3338/5000. 0.1225 s / img. ETA=0:04:16
[32m[01/30 22:03:01 d2.evaluation.evaluator]: [0mInference done 3367/5000. 0.1225 s / img. ETA=0:04:12
[32m[01/30 22:03:06 d2.evaluation.evaluator]: [0mInference done 3401/5000. 0.1225 s / img. ETA=0:04:07
[32m[01/30 22:03:11 d2.evaluation.evaluator]: [0mInference done 3433/5000. 0.1226 s / img. ETA=0:04:02
[32m[01/30 22:03:17 d2.evaluation.evaluator]: [0mInference done 3464/5000. 0.1226 s / img. ETA=0:03:57
[32m[01/30 22:03:22 d2.evaluation.evaluator]: [0mInference done 3499/5000. 0.1226 s / img. ETA=0:03:52
[32m[01/30 22:03:27 d2.evaluation.evaluator]: [0mInference done 3532/5000. 0.1226 s / img. ETA=0:03:47
[32m[01/30 22:03:32 d2.evaluation.evaluator]: [0mInference done 3564/5000. 0.1226 s / img. ETA=0:03:42
[32m[01/30 22:03:37 d2.evaluation.evaluator]: [0mInference done 3595/5000. 0.1227 s / img. ETA=0:03:37
[32m[01/30 22:03:42 d2.evaluation.evaluator]: [0mInference done 3628/5000. 0.1227 s / img. ETA=0:03:32
[32m[01/30 22:03:47 d2.evaluation.evaluator]: [0mInference done 3661/5000. 0.1227 s / img. ETA=0:03:27
[32m[01/30 22:03:52 d2.evaluation.evaluator]: [0mInference done 3693/5000. 0.1228 s / img. ETA=0:03:22
[32m[01/30 22:03:57 d2.evaluation.evaluator]: [0mInference done 3727/5000. 0.1228 s / img. ETA=0:03:17
[32m[01/30 22:04:02 d2.evaluation.evaluator]: [0mInference done 3757/5000. 0.1228 s / img. ETA=0:03:12
[32m[01/30 22:04:07 d2.evaluation.evaluator]: [0mInference done 3789/5000. 0.1229 s / img. ETA=0:03:07
[32m[01/30 22:04:12 d2.evaluation.evaluator]: [0mInference done 3821/5000. 0.1229 s / img. ETA=0:03:02
[32m[01/30 22:04:18 d2.evaluation.evaluator]: [0mInference done 3855/5000. 0.1229 s / img. ETA=0:02:57
[32m[01/30 22:04:23 d2.evaluation.evaluator]: [0mInference done 3888/5000. 0.1229 s / img. ETA=0:02:52
[32m[01/30 22:04:28 d2.evaluation.evaluator]: [0mInference done 3921/5000. 0.1229 s / img. ETA=0:02:47
[32m[01/30 22:04:33 d2.evaluation.evaluator]: [0mInference done 3953/5000. 0.1229 s / img. ETA=0:02:42
[32m[01/30 22:04:38 d2.evaluation.evaluator]: [0mInference done 3988/5000. 0.1229 s / img. ETA=0:02:36
[32m[01/30 22:04:43 d2.evaluation.evaluator]: [0mInference done 4018/5000. 0.1230 s / img. ETA=0:02:32
[32m[01/30 22:04:48 d2.evaluation.evaluator]: [0mInference done 4050/5000. 0.1230 s / img. ETA=0:02:27
[32m[01/30 22:04:53 d2.evaluation.evaluator]: [0mInference done 4081/5000. 0.1230 s / img. ETA=0:02:22
[32m[01/30 22:04:58 d2.evaluation.evaluator]: [0mInference done 4113/5000. 0.1230 s / img. ETA=0:02:17
[32m[01/30 22:05:03 d2.evaluation.evaluator]: [0mInference done 4146/5000. 0.1231 s / img. ETA=0:02:12
[32m[01/30 22:05:08 d2.evaluation.evaluator]: [0mInference done 4175/5000. 0.1231 s / img. ETA=0:02:08
[32m[01/30 22:05:14 d2.evaluation.evaluator]: [0mInference done 4208/5000. 0.1231 s / img. ETA=0:02:02
[32m[01/30 22:05:19 d2.evaluation.evaluator]: [0mInference done 4241/5000. 0.1231 s / img. ETA=0:01:57
[32m[01/30 22:05:24 d2.evaluation.evaluator]: [0mInference done 4272/5000. 0.1231 s / img. ETA=0:01:53
[32m[01/30 22:05:29 d2.evaluation.evaluator]: [0mInference done 4304/5000. 0.1232 s / img. ETA=0:01:48
[32m[01/30 22:05:34 d2.evaluation.evaluator]: [0mInference done 4338/5000. 0.1232 s / img. ETA=0:01:42
[32m[01/30 22:05:39 d2.evaluation.evaluator]: [0mInference done 4373/5000. 0.1232 s / img. ETA=0:01:37
[32m[01/30 22:05:44 d2.evaluation.evaluator]: [0mInference done 4404/5000. 0.1232 s / img. ETA=0:01:32
[32m[01/30 22:05:49 d2.evaluation.evaluator]: [0mInference done 4437/5000. 0.1232 s / img. ETA=0:01:27
[32m[01/30 22:05:54 d2.evaluation.evaluator]: [0mInference done 4470/5000. 0.1232 s / img. ETA=0:01:22
[32m[01/30 22:05:59 d2.evaluation.evaluator]: [0mInference done 4501/5000. 0.1232 s / img. ETA=0:01:17
[32m[01/30 22:06:04 d2.evaluation.evaluator]: [0mInference done 4535/5000. 0.1232 s / img. ETA=0:01:12
[32m[01/30 22:06:09 d2.evaluation.evaluator]: [0mInference done 4566/5000. 0.1233 s / img. ETA=0:01:07
[32m[01/30 22:06:14 d2.evaluation.evaluator]: [0mInference done 4598/5000. 0.1233 s / img. ETA=0:01:02
[32m[01/30 22:06:20 d2.evaluation.evaluator]: [0mInference done 4631/5000. 0.1233 s / img. ETA=0:00:57
[32m[01/30 22:06:25 d2.evaluation.evaluator]: [0mInference done 4662/5000. 0.1234 s / img. ETA=0:00:52
[32m[01/30 22:06:30 d2.evaluation.evaluator]: [0mInference done 4695/5000. 0.1234 s / img. ETA=0:00:47
[32m[01/30 22:06:35 d2.evaluation.evaluator]: [0mInference done 4726/5000. 0.1234 s / img. ETA=0:00:42
[32m[01/30 22:06:40 d2.evaluation.evaluator]: [0mInference done 4759/5000. 0.1234 s / img. ETA=0:00:37
[32m[01/30 22:06:45 d2.evaluation.evaluator]: [0mInference done 4792/5000. 0.1234 s / img. ETA=0:00:32
[32m[01/30 22:06:50 d2.evaluation.evaluator]: [0mInference done 4823/5000. 0.1234 s / img. ETA=0:00:27
[32m[01/30 22:06:55 d2.evaluation.evaluator]: [0mInference done 4857/5000. 0.1234 s / img. ETA=0:00:22
[32m[01/30 22:07:00 d2.evaluation.evaluator]: [0mInference done 4890/5000. 0.1235 s / img. ETA=0:00:17
[32m[01/30 22:07:05 d2.evaluation.evaluator]: [0mInference done 4920/5000. 0.1235 s / img. ETA=0:00:12
[32m[01/30 22:07:10 d2.evaluation.evaluator]: [0mInference done 4953/5000. 0.1235 s / img. ETA=0:00:07
[32m[01/30 22:07:16 d2.evaluation.evaluator]: [0mInference done 4986/5000. 0.1235 s / img. ETA=0:00:02
[32m[01/30 22:07:18 d2.evaluation.evaluator]: [0mTotal inference time: 0:12:56.794087 (0.155514 s / img per device, on 1 devices)
[32m[01/30 22:07:18 d2.evaluation.evaluator]: [0mTotal inference pure compute time: 0:10:17 (0.123527 s / img per device, on 1 devices)
[32m[01/30 22:07:20 d2.evaluation.coco_evaluation]: [0mPreparing results for COCO format ...
[32m[01/30 22:07:20 d2.evaluation.coco_evaluation]: [0mSaving results to ./output/inference/coco_instances_results.json
[32m[01/30 22:07:21 d2.evaluation.coco_evaluation]: [0mEvaluating predictions ...
Loading and preparing results...
DONE (t=0.14s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=33.41s).
Accumulating evaluation results...
DONE (t=4.61s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.410
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.615
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.449
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.249
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.331
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.521
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.546
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.362
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.581
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.683
[32m[01/30 22:07:59 d2.evaluation.coco_evaluation]: [0mEvaluation results for bbox: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 40.983 | 61.531 | 44.915 | 24.867 | 43.873 | 53.332 |
[32m[01/30 22:07:59 d2.evaluation.coco_evaluation]: [0mPer-category bbox AP: 
| category      | AP     | category     | AP     | category       | AP     |
|:--------------|:-------|:-------------|:-------|:---------------|:-------|
| person        | 55.298 | bicycle      | 30.191 | car            | 45.319 |
| motorcycle    | 42.431 | airplane     | 63.266 | bus            | 64.268 |
| train         | 62.870 | truck        | 35.587 | boat           | 26.851 |
| traffic light | 27.773 | fire hydrant | 66.343 | stop sign      | 67.090 |
| parking meter | 45.529 | bench        | 24.565 | bird           | 36.061 |
| cat           | 65.330 | dog          | 58.595 | horse          | 57.380 |
| sheep         | 50.692 | cow          | 53.743 | elephant       | 59.738 |
| bear          | 70.151 | zebra        | 65.603 | giraffe        | 65.381 |
| backpack      | 17.059 | umbrella     | 38.064 | handbag        | 15.099 |
| tie           | 33.637 | suitcase     | 37.302 | frisbee        | 64.712 |
| skis          | 24.532 | snowboard    | 34.820 | sports ball    | 47.006 |
| kite          | 41.695 | baseball bat | 28.480 | baseball glove | 35.169 |
| skateboard    | 50.463 | surfboard    | 36.813 | tennis racket  | 46.433 |
| bottle        | 39.662 | wine glass   | 35.924 | cup            | 41.291 |
| fork          | 33.533 | knife        | 18.536 | spoon          | 17.757 |
| bowl          | 42.382 | banana       | 23.846 | apple          | 21.321 |
| sandwich      | 34.833 | orange       | 30.510 | broccoli       | 22.423 |
| carrot        | 21.232 | hot dog      | 33.351 | pizza          | 50.907 |
| donut         | 44.416 | cake         | 34.571 | chair          | 27.508 |
| couch         | 42.475 | potted plant | 26.177 | bed            | 41.545 |
| dining table  | 27.836 | toilet       | 58.175 | tv             | 55.274 |
| laptop        | 59.256 | mouse        | 64.148 | remote         | 30.946 |
| keyboard      | 51.726 | cell phone   | 35.781 | microwave      | 54.767 |
| oven          | 33.342 | toaster      | 42.398 | sink           | 37.693 |
| refrigerator  | 54.862 | book         | 14.838 | clock          | 49.665 |
| vase          | 37.949 | scissors     | 26.213 | teddy bear     | 44.923 |
| hair drier    | 1.074  | toothbrush   | 22.270 |                |        |
Loading and preparing results...
DONE (t=1.65s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *segm*
DONE (t=37.73s).
Accumulating evaluation results...
DONE (t=4.37s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.372
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.586
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.399
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.186
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.395
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.533
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.307
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.476
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.497
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.313
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.530
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.644
[32m[01/30 22:08:48 d2.evaluation.coco_evaluation]: [0mEvaluation results for segm: 
|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |
|:------:|:------:|:------:|:------:|:------:|:------:|
| 37.169 | 58.598 | 39.878 | 18.633 | 39.492 | 53.298 |
[32m[01/30 22:08:48 d2.evaluation.coco_evaluation]: [0mPer-category segm AP: 
| category      | AP     | category     | AP     | category       | AP     |
|:--------------|:-------|:-------------|:-------|:---------------|:-------|
| person        | 47.659 | bicycle      | 17.969 | car            | 41.815 |
| motorcycle    | 32.986 | airplane     | 49.252 | bus            | 63.667 |
| train         | 61.038 | truck        | 35.068 | boat           | 23.022 |
| traffic light | 26.765 | fire hydrant | 62.378 | stop sign      | 66.174 |
| parking meter | 45.015 | bench        | 17.275 | bird           | 30.338 |
| cat           | 66.854 | dog          | 57.179 | horse          | 41.555 |
| sheep         | 43.681 | cow          | 46.896 | elephant       | 55.802 |
| bear          | 69.355 | zebra        | 56.278 | giraffe        | 51.522 |
| backpack      | 16.440 | umbrella     | 44.650 | handbag        | 14.933 |
| tie           | 31.189 | suitcase     | 39.446 | frisbee        | 62.388 |
| skis          | 3.222  | snowboard    | 21.955 | sports ball    | 46.843 |
| kite          | 30.874 | baseball bat | 24.689 | baseball glove | 38.559 |
| skateboard    | 31.492 | surfboard    | 31.171 | tennis racket  | 53.682 |
| bottle        | 38.238 | wine glass   | 30.948 | cup            | 41.307 |
| fork          | 15.283 | knife        | 12.811 | spoon          | 12.155 |
| bowl          | 40.012 | banana       | 19.467 | apple          | 20.167 |
| sandwich      | 36.739 | orange       | 30.311 | broccoli       | 21.661 |
| carrot        | 18.588 | hot dog      | 27.428 | pizza          | 50.275 |
| donut         | 45.267 | cake         | 35.038 | chair          | 18.140 |
| couch         | 36.135 | potted plant | 22.723 | bed            | 32.042 |
| dining table  | 16.106 | toilet       | 57.366 | tv             | 57.325 |
| laptop        | 59.190 | mouse        | 64.251 | remote         | 28.451 |
| keyboard      | 50.812 | cell phone   | 34.009 | microwave      | 55.995 |
| oven          | 31.137 | toaster      | 43.311 | sink           | 35.765 |
| refrigerator  | 57.000 | book         | 10.240 | clock          | 50.323 |
| vase          | 36.551 | scissors     | 20.594 | teddy bear     | 43.648 |
| hair drier    | 0.636  | toothbrush   | 14.988 |                |        |
[32m[01/30 22:08:48 d2.engine.defaults]: [0mEvaluation results for coco_2017_val in csv format:
[32m[01/30 22:08:48 d2.evaluation.testing]: [0mcopypaste: Task: bbox
[32m[01/30 22:08:48 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl
[32m[01/30 22:08:48 d2.evaluation.testing]: [0mcopypaste: 40.9835,61.5310,44.9146,24.8667,43.8730,53.3317
[32m[01/30 22:08:48 d2.evaluation.testing]: [0mcopypaste: Task: segm
[32m[01/30 22:08:48 d2.evaluation.testing]: [0mcopypaste: AP,AP50,AP75,APs,APm,APl
[32m[01/30 22:08:48 d2.evaluation.testing]: [0mcopypaste: 37.1689,58.5980,39.8783,18.6332,39.4916,53.2981
